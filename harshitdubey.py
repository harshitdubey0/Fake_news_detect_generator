# -*- coding: utf-8 -*-
"""harshitdubey

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjbM9NtGLl0mZCeK0tpB3EcDODJNlVcJ
"""

import streamlit as st
import pandas as pd
import re
# NLTK imports are removed as preprocessing is handled by the pre-trained model
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# from nltk.tokenize import word_tokenize

# sklearn imports are removed as pre-trained model handles it
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.linear_model import LogisticRegression

from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel
import torch
import os
import requests


# --- Configuration for Data URLs (No longer needed for detector data) ---
# TRUE_NEWS_URL and FAKE_NEWS_URL are commented out as they are not used by the pre-trained detector
# TRUE_NEWS_URL = "https://drive.google.com/uc?export=download&id=1t1M4CXDfifk1EP7K3_e6vh9I-yslfLTn"
# FAKE_NEWS_URL = "https://drive.google.com/uc?export=download&id=1tam_5hrqy1CcZ12mMGlBTw7y-dt3NjWj"


# --- NEW: Load Pre-trained Detector Pipeline (Using a more robust model) ---
@st.cache_resource
def load_pretrained_detector_pipeline():
    """Loads a pre-trained text classification model from Hugging Face."""
    st.spinner("Loading pre-trained Detector model... This may take a moment.")
    try:
        # Using a general sentiment analysis model as a robust text classifier.
        # This model is widely used and generally has fewer download issues.
        # It typically outputs 'POSITIVE' or 'NEGATIVE'
        detector_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
        st.success("Pre-trained Detector loaded!")
        return detector_pipeline
    except requests.exceptions.RequestException as e:
        st.error(f"Network error downloading Detector model: {e}")
        st.warning("This often happens due to temporary network issues or rate limiting from Hugging Face.")
        st.warning("Please try refreshing the app or deploying again later.")
        return None
    except Exception as e:
        st.error(f"Error loading pre-trained Detector model: {e}")
        st.warning("Ensure you have internet access and that the model name is correct.")
        return None

# --- Original Generator Pipeline (remains the same) ---
@st.cache_resource
def load_generator_pipeline():
    """Loads the pre-trained GPT-2 text generation pipeline."""
    st.spinner("Loading GPT-2 generator model... This may take a moment (first time download).")
    try:
        device = "cpu" # Force CPU for Streamlit Cloud deployment
        generator_pipeline = pipeline(
            'text-generation',
            model='gpt2',
            tokenizer='gpt2',
            device=0 if torch.cuda.is_available() else -1
        )
        st.success(f"GPT-2 generator loaded! Running on: {device.upper()}")
        return generator_pipeline
    except requests.exceptions.RequestException as e:
        st.error(f"Network error downloading GPT-2 model: {e}")
        st.warning("This often happens due to temporary network issues or rate limiting from Hugging Face.")
        st.warning("Please try refreshing the app or deploying again later.")
        return None
    except Exception as e:
        st.error(f"Error loading generator model: {e}")
        st.warning("Ensure you have internet access for the initial download of GPT-2.")
        return None

# --- Main Streamlit App Layout and Theming ---
st.set_page_config(
    page_title="Fake News AI",
    page_icon="üïµÔ∏è",
    layout="wide",
    initial_sidebar_state="collapsed"
)

st.title("üïµÔ∏è Fake News Detector & News Generator ‚úçÔ∏è")
st.markdown("""
    <style>
    .stApp {
        background-color: #0e1117;
        color: #e0e0e0;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 12px;
        border: none;
        padding: 10px 24px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 4px 2px;
        cursor: pointer;
        transition-duration: 0.4s;
        box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2), 0 6px 20px 0 rgba(0,0,0,0.19);
    }
    .stButton>button:hover {
        background-color: #45a049;
        box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19);
    }
    .stTextArea textarea {
        background-color: #1a1c20;
        color: #e0e0e0;
        border-radius: 8px;
        border: 1px solid #333;
    }
    .stTextInput input {
        background-color: #1a1c20;
        color: #e0e0e0;
        border-radius: 8px;
        border: 1px solid #333;
    }
    .stSlider .stSliderHandle {
        background-color: #4CAF50;
    }
    .stSlider .stSliderTrack {
        background-color: #333;
    }
    .stAlert {
        border-radius: 8px;
    }
    .stAlert.error {
        background-color: #ff4d4f;
        color: white;
    }
    .stAlert.success {
        background-color: #52c41a;
        color: white;
    }
    .stAlert.warning {
        background-color: #faad14;
        color: white;
    }
    a {
        color: #1890ff;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    </style>
    """, unsafe_allow_html=True)


st.markdown("""
    Welcome to the Fake News AI prototype! This application demonstrates two key aspects of combating misinformation:
    identifying fake news and understanding how synthetic text can be generated.
""")

# --- Load Models on App Startup ---
# Load the pre-trained detector pipeline
detector_pipeline = load_pretrained_detector_pipeline()

# Load the generator pipeline
generator_pipeline = load_generator_pipeline()

# --- Fake News Detector Section ---
st.header("üîç Fake News Detector")
st.markdown("Enter any news text below to check if it's likely real or fake (using a general sentiment model).") # Updated description

detector_input = st.text_area("News Article Text:", height=200, key="detector_input")

if st.button("Detect News", key="detect_button"):
    if detector_input and detector_pipeline:
        with st.spinner("Analyzing news text..."):
            result = detector_pipeline(detector_input)[0]
            label = result['label'] # e.g., 'POSITIVE', 'NEGATIVE'
            score = result['score'] # Confidence score

            # Map labels to human-readable format based on the sentiment model's output
            if label == 'POSITIVE':
                st.success(f"‚úÖ This text is likely **POSITIVE.** (Confidence: {score:.2f})")
            elif label == 'NEGATIVE':
                st.error(f"üö® This text is likely **NEGATIVE.** (Confidence: {score:.2f})")
            else:
                st.info(f"Prediction: {label} (Confidence: {score:.2f})")
    elif not detector_input:
        st.warning("Please enter some text to detect.")
    else:
        st.warning("Detector is not ready. Please check for errors above.")

st.markdown("---")

# --- News Generator Section ---
st.header("‚úçÔ∏è News Generator")
st.markdown("Enter a prompt, and the AI will generate text for you. This demonstrates general text generation capabilities.")

generator_prompt = st.text_input("Enter a prompt (e.g., 'Breaking news from the capital:', 'Scientists discover...'):", key="generator_prompt")
max_gen_length = st.slider("Maximum generated text length:", 50, 500, 150, key="max_length_slider")

if st.button("Generate Text", key="generate_button"):
    if generator_prompt and generator_pipeline:
        with st.spinner("Generating text..."):
            generated_text_output = generator_pipeline(
                generator_prompt,
                max_length=max_gen_length,
                num_return_sequences=1,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.95
            )[0]['generated_text']
        st.text_area("Generated Text:", generated_text_output, height=250, key="generated_output")
    elif not generator_prompt:
        st.warning("Please enter a prompt to generate text.")
    else:
        st.warning("Generator is not ready. Please check for errors above.")

st.markdown("---")
st.caption("Project by Harshit Dubey")
st.caption("Note: The generator uses a base GPT-2 model. For specialized 'fake news' generation, further fine-tuning would be required.")

# --- Connect with Me Section ---
st.markdown("<br>", unsafe_allow_html=True)
st.subheader("Connect with Harshit Dubey")

col1, col2, col3 = st.columns([1,1,1])

with col1:
    st.markdown(
        """
        <a href="https://www.instagram.com/harshitdubey00?igsh=OTJuOGNvbWI4c2Fj" target="_blank" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/a/a5/Instagram_icon.png" alt="Instagram" width="30" height="30" style="vertical-align: middle; margin-right: 5px;">
            <span style="font-size: 18px; color: #E1306C;">Instagram</span>
        </a>
        """,
        unsafe_allow_html=True
    )

with col2:
    st.markdown(
        """
        <a href="https://github.com/harshitdubey0" target="_blank" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub" width="30" height="30" style="vertical-align: middle; margin-right: 5px;">
            <span style="font-size: 18px; color: #6e5494;">GitHub</span>
        </a>
        """,
        unsafe_allow_html=True
    )

with col3:
    st.markdown(
        """
        <a href="mailto:harshitdubey11578@gmail.com" style="text-decoration: none;">
            <img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Gmail_Icon.png" alt="Email" width="30" height="30" style="vertical-align: middle; margin-right: 5px;">
            <span style="font-size: 18px; color: #DB4437;">Email</span>
        </a>
        """,
        unsafe_allow_html=True
    )